* course schedule
** unit 1 web crawling bots - 2.5 weeks
*** May 29, session 1: intro to Python & web scraping with bs4
**** syllabus & python installations (15m)
- check if python is 3+, ideally 3.9+
- install python with anaconda

  #+begin_src console

    python --version
    which python
    
  #+end_src

**** command line (30-45m)
- lesson: introduction to the command line
  - navigating
  - creating folders and files
  - using nano

Navigating

#+begin_src console
  # navigation
  cd
  cd ~
  cd ..
  cd /

  # listing and flags
  ls
  ls -a
  ls -l

  # creating files and folders
  mkdir botz
  touch test.md
  touch test.py

  # editing files
  nano test.md
  
#+end_src

Now, review lists and loops.

~nano test.py~

#+begin_src python
  # create a list of breakfast items
  # write a loop that prints the items
#+end_src

**** web scraping tools & ethics (30-45m)
- bs4, scrapy, selenium
- what is ethical, what is legal?
- read article on ICE,
  - quickwrite thoughts, discussion.

Web scraping tools:
- bs4
- scrapy
- selenium

Ethics and Legality
- to know if it's legal, check:
  - Terms of Service: If the terms say that reproduction or copying is
    not permitted, you cannot legally display any information you
    gather from the site. If the Terms of Service prohibits
    web-crawling or web-scraping, you should not use web-crawling or
    web-scraping on the site.
  - Is the information private? Determine whether the information
    you’re using could be considered “private.” Did you have to log-in
    to see the information? Is it personal in nature? If so, there may
    be additional restrictions on its use.
  - “robots.txt” file? Laws surrounding web-crawling and web-scraping
    prohibit you from bypassing this code.
- Computer Fraud and Abuse Act, 1986
  - not allowed to access a computer without authorization
  - doesn't define authorization
- /U.S. v Aaron Swartz/ 2010
  - Aaron Swartz created a program that automatically downloaded
    articles from JSTOR, an online library of academic literature,
    which went against the policies of the site. In creating this
    program, he used code to circumvent the restrictions to downloads
    on the site.
  - In 2011, a federal investigation was levied and, despite the fact
    that JSTOR expressed a wish to not press criminal charges, Swartz
    faced 13 federal charges and up to 50 years in prison.
- Copyright
  - Because U.S. copyright law does not protect facts or ideas (rather
    they protect the original, creative expression of those ideas),
    copyright protection of databases can be a little tricky.


ICE article
- read the article (5-10 minutes), taking some notes on what sticks out
  to you.
- respond to prompt (5-10 minutes)
- discuss (10-20 minutes)

#+begin_quote
It’s unclear how exactly any contractor might sniff out someone’s
“proclivity for violence.” The ICE document states only that the
contractor will use “social and behavioral sciences” and
“psychological profiles” to accomplish its automated threat detection.
#+end_quote

- How do you imagine they might achieve this goal of assessing
  someone's "proclivity for violence"? What data would they have to
  get, and how would they arrive to these conclusions from the data?
  No wrong answers!
- How do you think people might be able to avoid being tracked?
  Suggest some ideas.

**** BREAK
**** web scraping with bs4 (1.30m)

***** Crash course on bs4: NYT (50m)

#+begin_src
from bs4 import BeautifulSoup

import requests

import lxml

soup = BeautifulSoup(requests.get('https://nytimes.com').content, 'lxml')

soup.title

### demonstrate how to use the inspector tool to find different parts of the page

# use the find() function to search by element and class
soup.find('', class_ = '')

# use the .text element to pull out just the text from the element
soup.find('', class_ = '').text

#### how would I get all of the headlines? what is the method?

# use the find_all() method to get all of the headlines
headlines = soup.find_all('', class_ = '')

headlines

for i in headlines:
    print(i.text)

### how would I save this information to a list? 

titles = []
for i in headlines:
    titles.append(i.text)
    
titles

### challenge: use these tools to get the summary blurb for each article. 10-15 minutes & share.


### turn it to a function!
### write a function that scrapes all of the text from a
### webapge. Make it so you can pass different URLs into the function, and
### it will scrape a page for that URL. 

#+end_src

Advanced challenge: go to one of the searches. How would you scrape
the articles contained in the search results?
hint: you'll have to make a lot of soups
- write a loop that goes through each one.
- turns that page into soup, grabs the article data, appends it to the
  list

Time to explore websites to scrape data from. Check if they are
scrapable first. 

***** practice doing the same with a chatbot (20m/20m)
Use chatgpt or another chatbot of your choice to do the same as you
just did. Did it work? Why or why not?
  
**** homework: find 2 scrapable sites and explain why
Find 2 websites to scrape. Make sure if they are scrapable with bs4.
Why are you interested in this data? What could you do with it?

*** June 2, session 2: scrapy & the scrapy shell
**** share websites that you found
**** introduction to scrapy shell
***** setting up environment
Download and install scrapy. [[https://docs.scrapy.org/en/latest/intro/install.html#intro-install][Troubleshooting installtions]].

#+begin_src console
  conda install -c conda-forge scrapy
  pip install Scrapy
#+end_src

Introduction to scrapy shell

#+begin_src console
scrapy shell 'https://quotes.toscrape.com/page/1/'
response.css("title::text").get()
#+end_src

***** [[https://docs.scrapy.org/en/latest/intro/tutorial.html#extracting-data][Extracting data]] with css selectors

Syntax:

#+begin_src python
  # basic syntax for using element to get text
  # returns the entire element
  response.css("element::text")

  # getting the title elements
  response.css("title::text")

  # use dot to get classes
  # getting the quote elements
  response.css("span.text::text")

  # combine with get() to get just the text
  response.css("title::text").get()

  # and just the first instance
  response.css("span.text::text").get()

  # or with getall() to get a list
  response.css("span.text::text").getall()
#+end_src

***** narrowing down our content
- write a loop that prints out the elements that we want
  - looping through a subset of the page
  - using the print function
  - you will have to expand this loop to add author and links

#+begin_src python
# first just looping through to print
for i in response.css('div.view-list--content-container'):
    print(i.css('h4::text').get())
#+end_src 

**** scrapy project
***** starting new scrapy project

Install VS Code

Following tutorial on scrapy's [[https://docs.scrapy.org/en/latest/intro/tutorial.html][tutorial in the docs]]:

#+begin_src console
  # create your Scrapy project:
  scrapy startproject project_name
  cd project_name

  # see the directory structure
  tree
#+end_src

Create new spider manually (copy/paste code from [[https://docs.scrapy.org/en/latest/intro/tutorial.html#extracting-data-in-our-spider][extracting data in
our spider]]).

***** run a spider!

Insert your own variables and selectors.

#+begin_src python

  import scrapy

  class LatestSpider(scrapy.Spider):
      name = "latest"
      start_urls = [
	  "https://www.heritage.org/",
      ]

      def parse(self, response):
	  for i in response.css("div.view-list--content-container"):
	      yield {
		  "title": i.css('h4::text').get(),
	      }
            
#+end_src

Classes:
- classes are like templates, which you can customize.
  - building from a base c
  - contain properties and functions.
- ~LatestSpider~ class builds on the ~Spider~ class.
- check out the [[https://docs.scrapy.org/en/latest/topics/spiders.html][Spider class in the docs]].

Run the spider:

~scrapy crawl latest~

Then store the scraped data:

~scrapy crawl quotes -o latest.json~

**** (if time) explore a website you're interested in to get selectors
**** BREAK
**** introduce book, read introduction together, discuss
**** homework: reading response /Compost Engineers/ chapters 1 & 2
Joana Varon and Lucía Egaña Rojas. Chapters 1 & 2 from /Compost
Engineers and Sus Saberes Lentos: A Manifest for Regenerative
Technologies/. Coding Rights, 2024,
https://codingrights.org/docs/compost_engineers.pdf.

Prompt: Pick an idea from the reading that interests you (either
because you agree with it, disagree with it, or are otherwise provoked
by it) and explain why.

*** June 5, session 3: scrapy continued
**** share homework, discuss reading

Joana Varon & Lucia Egana, /The compost engineers and/sus saberes
lentos:a manifest for regenerative technologies/, Coding Rights, 2024.

Main ideas:
- the term "artificial intelligence" carries with it an imaginary that
  is shaped by war, colonialism, and patriarchy.
  - patriarchial ideas of domination.
  - colonialism: the englightenment principles of man as a rational
    animal supported and justified expansion and conquest over
    "primitive" peoples and lands.
    - Civilized vs primitive, master vs servant, and other dichotomies.
    - fantasies of discovery and appropriation. 
    - extend into today with the idea of the AI being something that
      can serve us, that we should dominate, but that can also
      dominate us.
- the tech pratices we have now are descendant from these imaginaries:
  of colonialism and patriarchy, they are influenced and materialized
  by them.
  - extrativist technology
    - what counts as "open" (for taking)
  - socio-environmental harm
  - the cloud as dematerialized
  - labor as dematerialized
  - universalizing the white male subject
  - efficiency above all, what counts as the "cost"
  - idea of progress over time, positivism vs intuitive knowlege
- How can we bring back AI technology and use it as tools, not
  solutions? (30)
  - "constitutive outside" - the outside of an idea that defines the
    idea. 

**** practice time with scrapy
Walk through script for crawling articles based on "gender" in
"commentary" filter.

Note:
- how the start URL is formatted
- how the two functions are doing different things
- how they are calling each other from within the function

#+begin_src python
import scrapy

class GenderSpider(scrapy.Spider):
    name = "gender"
    start_urls = [
        "https://www.heritage.org/gender?f%5B0%5D=content_type%3Acommentary",
    ]

    def parse(self, response):
        article_page_links = response.css("div.result-card__info-wrapper a::attr('href')")
        yield from response.follow_all(article_page_links, self.parse_article)

        next_page = response.css('li.pager__item a::attr("href")').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)

    def parse_article(self, response):
        yield {
            'title': response.css('h1.headline::text').get(),
            'takeaways':response.css('div.key-takeaways__takeaway p::text').getall(),
            'text': response.css('div.article__body-copy div p::text').getall()
        }

#+end_src

**** BREAK
**** pratice & 1:1s
**** homework: /Compost Engineers/ chapters 3 & 4
Joana Varon and Lucía Egaña Rojas. Chapters 3 & 4 from /Compost
Engineers and Sus Saberes Lentos: A Manifest for Regenerative
Technologies/. Coding Rights, 2024,
https://codingrights.org/docs/compost_engineers.pdf.

Prompt: From the authors' proposals, what do you find useful or
surprising, and what do you have doubts about?

*** June 9, session 4: XHR
**** share homework, discuss reading
Where does the book end up? What's the proposal?

A proposal for a garden being cultivated according to certain
principles:
- ecological principles, indigenous studies principles, feminist
  principles.
  - /The Mushroom at the End of the world/
- symbiosis, regeneration, non-prioritizing of the human,
  contamination. 

The idea is that there is knowledge in the cultivating of a garden
that they can learn and apply to digital tools.
- the idea of "knowledge" vs "intelligence". 

**** finding undocumented APIs
Yin, Leon. Finding Undocumented APIs. 24 Feb. 2023,
https://inspectelement.org/apis.html#tutorial.

What is an API? A way for you to get data from a source.
- An application that holds and distributes the data you want.
- A language for interacting with that particular application

Documented vs undocumented APIs
- documented APIs: the MET API, for example.
- undocumented: no documentation. They operate under the hood,
  grabbing data to populate websites.

Undocumented APIs do not engage with the web page, the HTML. They
engage with the network, with the request being made over the network.
- they are the middle man, fulfilling requests from the code (usually
  javascript) to get data for the page.
 
***** the MET API (review)
- the anatomy of a request
  - root, path, query
- the response object
  - parse into json
  - dictionary syntax for accessing elements

***** XHR tutorial

We'll see how Amazon.com autocomplete search suggestions work.

We will scrape Amazon search results by reverse-engineering how
requests are made, and use it to collect structured data at scale.

****** 1. First open the developer console.

See how on Chrome or Firefox here.

One way to get to the dev tools it to right-click and “Inspect” an
element on the page.

****** 2. Click the “Network” tab.

This section of the dev tools is used to monitor network requests.

Everything on a page is retrieved from some outside source, likely a
server. This includes things like images embedded on the page,
JavaScript code running in the background, and all the bits of
“content” that populate the page before us.

Using the Network tab, we can find out how this information is
requested from a server, and intercept the response before it is
rendered on the page.

****** 3. Filter requests by fetch/XHR

This will reveal only API calls made to servers. This includes
internal servers that are hosted by the website we’re inspecting, as
well as external servers. The latter often includes third-party
trackers used in adtech, and verification services to authenticate
user behavior.

You might see quite a few network requests that were loaded onto the
page. Look at "Domain" to narrow down where requests were sent, and
whether the names are telling of the purpose of the request. In this
example, notice that a request was sent to the "Domain"
completion.amazon.com.

Look at the request's "Response" attributes.

****** 4. Copy as cURL

If you find an HTTP request that returns a response with useful
information you can start to reverse-engineer it. To do that, we can
isolate it by right-clicking the HTTP request and selecting “copy as
cURL”. (cURL stands for client URL, and is a tool used to transfer
data across networks.)

You can paste this directly in your command line. Or, you can convert
it to python to run as a script. 

****** 5. Curl to requests

We can use a site like curlconverter.com to convert the cURL we copied
into a reusable API call. In this example, we use the default
conversion to a Python requests script. You can do the same for any
language and framework.

Here is what the converted cURL looks like after being converted to a
Python request:

****** 6. copy and paste one by one into a notebook, run it

****** 7. port to a function

#+begin_src python
import pandas as pd
import time

def search_suggestions(keyword):
    """
    Get autocompleted search suggestions for a `keyword` search on Amazon.com.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Accept-Language': 'en-US,en;q=0.5',
    }

    params = {
        'prefix': keyword,
        'suggestion-type': [
            'WIDGET',
            'KEYWORD',
        ],
        'alias': 'aps',
        'plain-mid': '1',
    }

    response = requests.get('https://completion.amazon.com/api/2017/suggestions',
                            params=params, headers=headers)
    return response.json()
#+end_src

#+begin_src python
  search_suggestions('maga')
#+end_src

****** 8. Iterate through different searches, save to DF

Here we can set new input parameters in keyword, and make the an API
call using each keyword. Try changing some of the code (eg. the
keywords) and rerunning it to check your understanding.

#+begin_src python
    # Here are our inputs (what searches we'll get autocompleted)
  keywords = [
      'a', 'b', 'cookie', 'sock', 'zelda', '12'
  ]

  # Here we'll go through each input, get the suggestions, and then add the `suggestions` to a list.
  data = []
  for keyword in keywords:
      suggestions = search_suggestions(keyword)
      suggestions['search_word'] = keyword # keep track of the seed keyword
      time.sleep(1) # best practice to put some time between API calls.
      data.extend(suggestions['suggestions'])
#+end_src

See the results by looping through the "value" key

#+begin_src python
  for i in data:
      print(i['value'])
#+end_src

**** BREAK
**** check-in ideas for data you want to scrape
- at least 2 potential sources (or topics)
- the importance of doing environmental scans
**** individual practice & 1:1 
**** assignment (due June 19): analog tracking

# Analog Tracking Assignment

**Due**: June 19, 2025
**Length**: raw data log, & 1–2 pages of reflection 

## Overview

To create some context for building automated bots, you'll first
collect data the old-fashioned way—by hand, with your own attention as
the primary tool.

For this assignment, choose one real-world subject to observe
repeatedly over **5 days**. Try to chose days that are as close to
consecutive as possible, and as similar time each day as possible (for
example, Monday - Friday everyday at ~9am).

Your subject can be anything that changes over time: your meals, a
tree, a bus stop, a garden or park, etc. You can get creative, you can
be political, or you can be super simple. The goal is to practice
**systematic observation**, notice patterns, and reflect on the
experience of **manual data collection**.

You’ll keep a short daily journal and submit both your **raw
observations** and a **1–2 page reflection** on the experience.

---

## What to Do Each Day

Spend at least **10 minutes** each day observing your subject. As you
are doing so, answer the following prompts in a notebook or digital
doc:

1. Day
2. Time
3. Location
4. Observations
5. Feelings/Thoughts
6. Misc. Notes
7. (Optional) Include a photo, sketch, or visual note.

Feel free to add any categories that pertain to your specific case.
For example, (if you decided to get political) and are observing a
protest/strike, you can note the number of strikers or protesters that
day.

## Final Submission Instructions (due June 19th)

Compile a packet (in PDF format) that includes the following documents:

- Your daily tracking journal (ideally in a table format, but can also
  be bulleted list)
- A 1–2 page reflection answering:
  - What patterns did you see?
  - How did your tracking methods evolve?
  - What choices are you making about what counts as "data"? Why?
  - What would be easy or hard to automate about this process?

---

## Example Project: *Tree Watch*

**Subject**: A cherry blossom tree near my apartment  
**Location**: Riverside Park, NYC  
**Timeframe**: March 24–28, 10–10:15am daily

| Day | Time    | Location        | Observations                                   | Feelings/Thoughts                        | Misc. Notes                     |
|-----|---------|------------------|------------------------------------------------|------------------------------------------|----------------------------------|
| 1   | 10:00am | Riverside Park   | Buds just starting to open, ~3 visible         | Calm; surprised by how early bloom starts | Cloudy, ~48°F                    |
| 2   | 10:10am | Riverside Park   | 7–10 small flowers open; jogger passed by      | Focused; felt invisible while observing   | Took photo for reference         |
| 3   | 10:00am | Riverside Park   | 12+ flowers open; lots of bees around          | Curious; noticed sound of bees            | Warm and sunny (~61°F)           |
| 4   | 10:05am | Riverside Park   | Petals damp, a few fallen; no bees             | Dreary; hard to feel motivated            | Light rain; few people outside   |
| 5   | 10:15am | Riverside Park   | Many petals blown off; almost bare branches    | Bit sad; felt season change more deeply   | Kids nearby called it “snow”     |

---

*** June 12, session 5: selenium
**** agenda & review web scraping
Last day of our unit on web scraping, will be moving on to social
media next unit.

Where we are:
- thinking about data we are interested in, not necessarily what we
  will do with that data (next week, text processing), but a general
  topic and/or resource is good.
- choosing a tool to get the data we want.
- after today will have 4 tools: bs4, scrapy, xhr, selenium.
- also after today, will have a sense of other projects that are
  already out there.

So we will start with a review of web scraping methods, then we will
move on to selenium.
- Open ipython shell.
- bs4 - good for scraping individual websites that are static
- scrapy - good for crawling websites, multiple pages, that are
  static.
  - framework, has a lot more functionality (such as processing data
    in sophisticated ways)
- xhr - good for getting dynamic data, by intercepting network
  requests.
- selenium - good for getting dynamic data by simulating a browser.

Try to scrape doge.gov with all the above. 

**** introduction to selenium

***** installations
Install selenium
- run one of the following in your command line

~conda install selenium~
~pip install selenium~

Install driver
- we must use google chrome for selenium
- first, check your version of google chrome (open the app, select
  "chrome" at the top then "about chrome". Make a note of the first
  three numbers in the version.
- then, install the relevant /chromedriver/ from this page:
  https://googlechromelabs.github.io/chrome-for-testing/
  - check "about this mac" to find out if you need arm64 (M1+ / apple
    sillicon chip) or x64 (intel chip).
  - other pages with more information and versions:-
    https://sites.google.com/chromium.org/driver/getting-started?authuser=0
    https://googlechromelabs.github.io/chrome-for-testing/files
- once you download the file, unzip it, and move the folder to the
  same directory as the one you generally use for this class
  (containing your python files). 

***** selenium_lesson.py

#+begin_src python
### INTRODUCTION TO SELENIUM

# bs4 won't work
import requests
from bs4 import BeautifulSoup
import lxml
soup = BeautifulSoup(requests.get('https://doge.gov/').content, 'lxml')
soup.title

# niether will scrapy
from scrapy.selector import Selector
from scrapy.http import HtmlResponse
response = HtmlResponse(url="https://doge.gov", body=body, encoding="utf-8")
response = HtmlResponse(url="https://doge.gov", encoding="utf-8")
response.css('title::text').get()
response.css('title')

### to install selenium

# run one of the following in your command line:
# conda install selenium
# pip install selenium

### to install driver:

# first, check your version of google chrome (open the app, select
# "chrome" at the top then "about chrome". Make a note of the first
# three numbers in the version.
# second, install the relevant /chromedriver/ from this page:
# https://googlechromelabs.github.io/chrome-for-testing/
# check "about this mac" to find out if you need arm64 (M1+ / apple
# sillicon chip) or x64 (intel chip).
# finally, once you download the file, unzip it, and move the folder to the
# same directory as the one you generally use for this class
# (containing your python files). 

### using selenium:

# imports: driver, service, by
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

# variables to scrape site
url = 'https://doge.gov/'
d_path = '../chromedriver-mac-arm64/chromedriver'
service = Service(executable_path=d_path)
driver = webdriver.Chrome(service = service)

# scrape site
driver.get(url)

### our goal: to get the text from each post. check
### inspector for element for each post: for me, it is:
### div.border-2

# scraping element using "find_element" function, which takes two arguments
card = driver.find_element(By.CSS_SELECTOR, "div.border-2")

# see the card and just the text
card
card.text

# multiple elements with find_elements
cards = driver.find_elements(By.CSS_SELECTOR, "div.border-2")
cards
    
for i in cards:
    print(i.text)
    
#+end_src
**** BREAK
**** environmental scan research

Spend some time researching projects and apps that relate to your
topic and/or your dataset. Create a list of 3-5 projects, considering
the following:
- projects should be some kind of data gathering and/or analysis of
  your chosen data or resource.
  - example: if you are scraping recipes, you might include a top
    recipe website, and/or an article that compares recipe websites.
- include at least two bots/scrapers. 
  - check both a search engine and github.com.
  - try searching "[your topic] scraper python", or "[your topic] bot
    python"

Once you have your projects chosen, go through them one by one and
answer the following questions:
- what the project does accomplish? what appears to be its strengths?
- what does the project not do, what is outside the scope of the
  project? Try to identify a gap that may be filled by futher work on
  the topic/dataset. 
- if the project uses python:
  - where is it getting its data from?
  - what python libraries is it using? (check "import" statements at
    the top of the python files).

**** homework (due June 16): dataset brainstorm
Dataset brainstorm (due June 16)

What is the dataset you'd like to use as a basis for your final project? Where
would you get the data, and how would you go about collecting it?


Include a paragraph that explains your interest in the data, and
include a bulleted list with concrete steps for gathering the data.


** unit 2 social media bots - 2.5 weeks
*** June 16, session 5: spaCy
**** share dataset homework
**** intro to Python for cleaning text
- open up heritage spider dataset, share with everyone as well.
- show them how to open it up in pandas, and do some basic cleaning
- first, decide what needs to be cleaned
- ask co-pilot to flatten list of strings into a single string, to
  remove examples that are just author pages.
- review replace() method and using Regex??

**** the spaCy pipeline
**** NER in spaCy
**** pattern matching in spaCy???
**** BREAK
**** practice NER in dataset

*** June 19, session 6: transformers
**** how to run inference
**** how to fine-tune a model
**** BREAK
**** individual practice: fine-tune a model with own data
**** in class: read and explore COMPAS algorithm
- “Can You Make AI Fairer than a Judge? Play Our Courtroom Algorithm
  Game.” MIT Technology Review,
  https://www.technologyreview.com/2019/10/17/75285/ai-fairer-than-judge-criminal-risk-assessment-algorithm/.
- recommended:
  - Groves, Lara, et al. “Auditing Work: Exploring the New York City
    Algorithmic Bias Audit Regime.” The 2024 ACM Conference on
    Fairness, Accountability, and Transparency, ACM, 2024, pp.
    1107–20. DOI.org (Crossref),
    https://doi.org/10.1145/3630106.3658959.

**** in class: end user algorithmic audit
- goals:
  - opportunity to discuss what makes something toxic
  - opportunity to examine how models treat toxicity

***** indie-label

[[https://github.com/StanfordHCI/indie-label][IndieLabel]]

Installations:
- use conda to create an env with python 3.8
- then use pip to install the packages
- if coming up against cython and/or surprise package errors, see:
  - [[https://stackoverflow.com/questions/77490435/attributeerror-cython-sources][AttributeError: cython_sources [duplicate]]]
  - [[https://stackoverflow.com/questions/65679417/getting-errors-while-installing-surprise-package][Getting errors while installing Surprise package]]
  - [[https://stackoverflow.com/questions/79374322/importerror-cannot-import-name-cached-download-from-huggingface-hub][ImportError: cannot import name 'cached_download' from 'huggingface_hub']]

#+begin_src console

  # create a constraint to avoid cython
  echo "cython<3" > /tmp/constraint.txt 
  PIP_CONSTRAINT=/tmp/constraint.txt pip install -r requirements.txt

  # install scikit-surprise separately with conda
  conda install -c conda-forge scikit-surprise

  # error when running server.py
  pip install huggingface-hub==0.25.2

#+end_src

Audit instructions:
- open a blank document for note-taking
- complete the questionnaire to get your model in the "okay" range. 
  - as you complete the questionnaire, make notes of deciding factors
    that made you choose if something is toxic or not toxic.
- explore the "auditing" tab, make notes on your findings. 

Report:
- provide 2 examples of choices that were difficult or that made you
  second guess yourself
- why was the choice difficult?

**** source code walkthrough: bias evaluation projects
- goals: learn to read complicated python code/projects; adapt
  pre-existing code to your own purposes/experimentation. 
***** biases-llm-reference-letters
https://github.com/uclanlp/biases-llm-reference-letters/tree/main?tab=readme-ov-file

Useful functions to count how many times certain words appear, male or
female words.

Also uses spacy to create lists of male and female nouns and
adjectives. 

*** June 23, session 8: twitter bots
[[https://thepythoncode.com/article/make-a-twitter-bot-in-python][Twitter bot with Python]] tutorial

**** configuring environments

#+begin_src console

mkdir met_women
cd met_women

conda create --name met_women

# Activate the virtual environment:
# - MacOS/Linux
conda activate met_women

#+end_src

#+begin_src console
touch .env
nano .env
#+end_src

#+begin_src python
# Consumer Keys > API Key and Secret
API_KEY=<your-API-key>
API_SECRET=<your-API-secret>

# Authentication Tokens > Access Token and Secret
ACCESS_TOKEN=<your-access-token>
ACCESS_TOKEN_SECRET=<your-access-token-secret>
#+end_src

#+begin_src console
touch .gitignore
nano .gitignore
#+end_src

#+begin_src
__pycache__
.env*
#+end_src

#+begin_src console
% pip install tweepy, requests, python-dotenv
#+end_src

**** tweet.py

touch tweet.py
code tweet.py

#+begin_src python
import os
import tweepy
import requests
from dotenv import load_dotenv
from random import randint

load_dotenv()

API_KEY = os.getenv("API_KEY")
API_SECRET = os.getenv("API_SECRET")
ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = os.getenv("ACCESS_TOKEN_SECRET")

client = tweepy.Client(
    consumer_key=API_KEY,
    consumer_secret=API_SECRET,
    access_token=ACCESS_TOKEN,
    access_token_secret=ACCESS_TOKEN_SECRET
)

def tweet_women_fact(tweepy_client):

   print('fetching women from the MET...')
   r1 = requests.get("https://collectionapi.metmuseum.org/public/collection/v1/search?q=woman")

   parsed = r1.json()

   number = randint(1, 100)

   obj_id = parsed['objectIDs'][number]

   r2 = requests.get(f"https://collectionapi.metmuseum.org/public/collection/v1/objects/{obj_id}")

   parsed = r2.json()

   if parsed['title'] != '':
       text = f"Title: {parsed['title']}"
   else:
       text = f"Title: Unknown"
   if parsed['artistDisplayName'] != '':
       artist = f"Artist: {parsed['artistDisplayName']}"
   else:
       artist = 'Artist: Unknown'
   if parsed['artistGender'] != '':
       gender = parsed['artistGender']
   else:
       gender = 'Gender: Unknown'

   image = parsed['objectURL']

   tweet_text = f"{text}, {artist}, {gender} {image}"
   print('tweeting women from the MET...')

   tweepy_client.create_tweet(text=tweet_text)

tweet_women_fact(client)

#+end_src

**** deploying our bot

Tutorials:
- [[https://www.python-engineer.com/posts/run-python-github-actions/][Adding secrets to github actions]]
- Automating a Twitter bot with GitHub Actions ([[https://github.com/gabrielbelolima/ttBot][github repo]])
  - [[https://medium.com/@gabrielbelolima/a-step-by-step-tutorial-part-1-3-71a7a8444b0cAutomating][part 1/3]]
  - [[https://medium.com/@gabrielbelolima/automating-a-twitter-bot-with-github-actions-a-step-by-step-tutorial-part-2-3-ebc4968a10ec][part 2/3]]
  - [[https://medium.com/@gabrielbelolima/automating-a-twitter-bot-with-github-actions-a-step-by-step-tutorial-part-3-3-df5d15b1e339][part 3/3]]

#+begin_src console
  mkdir .github
  mkdir .github/workflows
  cd .github/workflows
  touch actions.yml
  code actions.yml
#+end_src

#+begin_src yml

on:
  schedule:
#    - cron: '0 * * * *' # at top of every hour
    - cron: '0 0 * * *' # At 00:00 every day
  
  push: 

jobs:
  build:

    runs-on: ubuntu-latest

    steps:

      - name: checkout repo content
        uses: actions/checkout@v2 # checkout the repository content

      - name: setup python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10' # install the python version needed

      - name: install python packages
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: run scrupt 
        run: python tweet.py
        env: 
            API_KEY: ${{ secrets.API_KEY }}
            API_SECRET: ${{ secrets.API_SECRET }}
            ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}
            ACCESS_TOKEN_SECRET: ${{ secrets.ACCESS_TOKEN_SECRET }}
  
#+end_src

**** BREAK
**** group project: social media bot tutorial
- choose a social media app, like instagram, tiktok, linkedin, or
  another app of your choice.
- research some tutorials for scraping and/or creating a bot for that
  app. Make sure the tutorial is recent (in the last year, at
  minimum).
- with a partner, create a tutorial that you will use to teach your
  classmates how to scrape or create a bot on that app.
- tutorial should be written in markdown format, with each step
  described clearly, and code blocks to include code examples.
- you will present the tutorial like a lesson, where you walk your
  classmates through the process of using the tool.
- 20-30 minutes lesson. 

Resources:
- Yin, Piotr Sapiezynski and Leon. Browser Automation. 11 June 2023,
  https://inspectelement.org/browser_automation.html.
- Instagrapi, [[https://www.youtube.com/watch?v=cW7kMeOUr20][instagrapi tutorial]]
- [[https://www.geeksforgeeks.org/make-an-instagram-bot-with-python/][Make an Instagram Bot With Python]], Geeks for Geeks

**** make a plan for actions steps by next class
*** (online) June 26, session 9: group projects
**** share progress, next steps
**** BREAK
**** breakout work sessions
**** mini-conferences with me
*** (online) June 30, session 10: group presentations & final projects
**** tutorial presentations
**** BREAK
**** introduction to git
**** introduce final project assignment
**** homework: project proposal
**** instagram
User: trans_phobia_
pass: supersecure

** unit 3 project workshops & presentations - 1 week
*** (online) July 3, final projects
- share progress
- project workshops
- mini conferences
*** (online) July 7, final project presentations
- presentations
** bank
**** scraping XHR
[[https://scrapism.lav.io/scraping-xhr/][Tutorial by Sam Lavigne]] on scraping Bing and Customs Border
Protection.

***** Exploring XHR from the command line

#+begin_src python
from bs4 import BeautifulSoup
import requests

query = "how can i"

url = (
    "https://www.bing.com/AS/Suggestions?pt=page.home&mkt=en-us&qry="
    + query
    + "&cp=9&csr=1&msbqf=false&pths=1&cvid=6AE710F2D778431589574CB8424EFF70"
)

response = requests.get(url)

response
dir(response)
response.text
response.content
response.json()

parsed = response.json()

# what kind of data structure?
# pull out the completions
parsed
parsed['s'][0]
parsed['s'][0]['q']
parsed['s'][1]['q']
parsed['s'][2]['q']

# write a loop that prints just the completions
for item in parsed['s']:
    print(item['q'])
#+end_src

***** Script for scraping XHR results.
- how & why to create a script
- how & why to write a function

Run the below. Pipe output through sort -u to sort the output of our
script and filter out duplicates.

#+begin_src console
  python bing_autocomplete.py | sort -u
#+end_src

#+begin_src python
from bs4 import BeautifulSoup
import requests

def auto_complete(query):
  url = (
      "https://www.bing.com/AS/Suggestions?pt=page.home&mkt=en-us&qry="
      + query
      + "&cp=10&cvid=B8D86CB090A240A196E4867715E40B15"
  )
  response = requests.get(url)
  soup = BeautifulSoup(response.text, "html.parser")
  items = soup.select("li")
  for item in items:
      print(item.text)

base_query = "How can I "
for letter in "abcdefghijklmnopqrstuvwxyz":
    auto_complete(base_query + letter)
    for letter2 in "abcdefghijklmnopqrstuvwxyz":
        auto_complete(base_query + letter + letter2)
  
#+end_src

**** individual activity: explore how to bypass blockers
Try out some of these strategies: 
- [[https://scrapeops.io/web-scraping-playbook/403-forbidden-error-web-scraping/][How To Solve 403 Forbidden Errors When Web Scraping]]
- [[https://www.zenrows.com/blog/bypass-cloudflare-python][How to Bypass Cloudflare in Python]]
- [[https://www.zenrows.com/blog/curl-bypass-cloudflare#set-real-http-headers][4 Methods to Bypass Cloudflare with cURL in 2025]]
**** homework: ACLU tech & privacy analysis write-up
Choose a recent topic from this page; write up analysis of what is
going on, and your opinion on the issue. How does the issue handle
privacy rights and ethical uses of data?
https://www.aclu.org/press-releases?issue=privacy-technology
