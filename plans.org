* course schedule
** unit 1 web crawling bots - 2 weeks
*** May 29, session 1: intro to Python & web scraping with bs4
**** python installations
- install python with anaconda
**** command line
- lesson: introduction to the command line
  - navigating
  - creating folders and files
  - using nano
  - curl requests
**** web scraping tools & ethics
- bs4, scrapy, selenium
- what is ethical, what is legal?
- read article on ICE,
  - quickwrite thoughts, discussion. 
**** BREAK
**** web scraping with bs4

***** Crash course on bs4: American Conservative articles

#+begin_src
from bs4 import BeautifulSoup

import requests

import lxml

soup = BeautifulSoup(requests.get('https://www.theamericanconservative.com/').content, 'lxml')

soup.title

### demonstrate how to use the inspector tool to find different parts of the page

# use the find() function to search by element and class
soup.find('', class_ = '')

# use the .text element to pull out just the text from the element
soup.find('', class_ = '').text

#### how would I get all of the headlines? what is the method?

# use the find_all() method to get all of the headlines
headlines = soup.find_all('', class_ = '')

headlines

for i in headlines:
    print(i.text)

### how would I save this information to a list? 

titles = []
for i in headlines:
    titles.append(i.text)
    
titles

### challenge: use these tools to get the summary blurb for each article. 10-15 minutes & share.

#+end_src

Challenge: write a function that scrapes all of the text from a
webapge. Make it so you can pass different URLs into the function, and
it will scrape a page for that URL. 

Advanced challenge: how would I scrape the article text?
hint: look at the URLs.
- make a list of URLs for every page.
- write a loop that goes through each one.
- turns that page into soup, grabs the article data, appends it to the
  list

Time to explore websites to scrape data from. Check if they are
scrapable first. 

**** (if time): freewrite/share
- Brainstorm what kind of data you are interested in working with.
  Look at some websites that may have that kind of data. Why are you
  interested in this data?
**** homework: find 2 scrapable sites
Find 2 websites to scrape. Make sure if they are scrapable with bs4.
Why are you interested in this data? What could you do with it?

*** June 2, session 2: scrapy & the scrapy shell
**** share websites that you found
**** introduction to scrapy shell
***** setting up environment
Create conda environment for scraping

Download and install scrapy. [[https://docs.scrapy.org/en/latest/intro/install.html#intro-install][Troubleshooting installtions]].

#+begin_src console
  conda install -c conda-forge scrapy
  pip install Scrapy
#+end_src

Introduction to scrapy shell

#+begin_src console
scrapy shell 'https://quotes.toscrape.com/page/1/'
response.css("title::text").get()
#+end_src

***** [[https://docs.scrapy.org/en/latest/intro/tutorial.html#extracting-data][Extracting data]] with css selectors

Syntax:

#+begin_src python
  # basic syntax for using element and class to get text
  # returns the entire element
  response.css("element.class::text")

  # getting the title elements
  response.css("title::text")

  # getting the quote elements
  response.css("span.text::text")

  # combine with get() to get just the text
  response.css("title::text").get()

  # and just the first instance
  response.css("span.text::text").get()

  # or with getall() to get a list
  response.css("span.text::text").getall()
#+end_src

Challenge: work to find the rest of the information on the page. Get
all the authors and the tags.

***** writing loops
- write a loop that saves our information to a loop. I will
  write the first two lines.
  - looping through a subset of the page.
  - using the print function
  - you will have to expand this loop.

#+begin_src python
  # first just looping through to print
  for quote in response.css("div.quote"):
       print(quote.css("span.text::text").get())

  # now saving it to a dictionary   
  quotes = {}
  for quote in response.css("div.quote"):
      quotes["text"] = quote.css("span.text::text").get()
#+end_src

Now, you will expand the loop to include author and tag information. 

**** (if time) explore a website you're interested in to get selectors
**** BREAK
**** scrapy project
***** starting new scrapy project

Install VS Code

Following tutorial on scrapy's [[https://docs.scrapy.org/en/latest/intro/tutorial.html][tutorial in the docs]]:

#+begin_src console
  # create your Scrapy project:
  scrapy startproject project_name
  cd project_name

  # see the directory structure
  tree
#+end_src

Create new spider manually (copy/paste code from [[https://docs.scrapy.org/en/latest/intro/tutorial.html#extracting-data-in-our-spider][extracting data in
our spider]]). 

#+begin_src python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/page/1/",
        "https://quotes.toscrape.com/page/2/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                "tags": quote.css("div.tags a.tag::text").getall(),
            }
#+end_src

Classes:
- classes are like templates, which you can customize.
- contain properties and functions.
- ~QuotesSpider~ class builds on the ~Spider~ class.
- check out the [[https://docs.scrapy.org/en/latest/topics/spiders.html][Spider class in the docs]].

Run the spider:

~scrapy crawl quotes~

Then store the scraped data:

~scrapy crawl quotes -O quotes.json~

**** (if time) individual activity: apply code to website from homework
- change the name, urls, and selectors.
- run the same command 

**** introduce book, read introduction together, discuss
**** homework: reading response /Compost Engineers/ chapters 1 & 2
Joana Varon and Lucía Egaña Rojas. Chapters 1 & 2 from /Compost
Engineers and Sus Saberes Lentos: A Manifest for Regenerative
Technologies/. Coding Rights, 2024,
https://codingrights.org/docs/compost_engineers.pdf.

Prompt: Pick an idea from the reading that interests you (either
because you agree with it, disagree with it, or are otherwise provoked
by it) and explain why.

*** June 5, session 3: blockers & XHR
**** share homework, discuss reading
**** scraping XHR
[[https://scrapism.lav.io/scraping-xhr/][Tutorial by Sam Lavigne]] on scraping Bing and Customs Border
Protection.

***** Exploring XHR from the command line

#+begin_src python
from bs4 import BeautifulSoup
import requests

query = "how can i"

url = (
    "https://www.bing.com/AS/Suggestions?pt=page.home&mkt=en-us&qry="
    + query
    + "&cp=9&csr=1&msbqf=false&pths=1&cvid=6AE710F2D778431589574CB8424EFF70"
)

response = requests.get(url)

response
dir(response)
response.text
response.content
response.json()

parsed = response.json()

# what kind of data structure?
# pull out the completions
parsed
parsed['s'][0]
parsed['s'][0]['q']
parsed['s'][1]['q']
parsed['s'][2]['q']

# write a loop that prints just the completions
for item in parsed['s']:
    print(item['q'])
#+end_src

***** Script for scraping XHR results.
- how & why to create a script
- how & why to write a function

Run the below. Pipe output through sort -u to sort the output of our
script and filter out duplicates.

#+begin_src console
  python bing_autocomplete.py | sort -u
#+end_src

#+begin_src python
from bs4 import BeautifulSoup
import requests

def auto_complete(query):
  url = (
      "https://www.bing.com/AS/Suggestions?pt=page.home&mkt=en-us&qry="
      + query
      + "&cp=10&cvid=B8D86CB090A240A196E4867715E40B15"
  )
  response = requests.get(url)
  soup = BeautifulSoup(response.text, "html.parser")
  items = soup.select("li")
  for item in items:
      print(item.text)

base_query = "How can I "
for letter in "abcdefghijklmnopqrstuvwxyz":
    auto_complete(base_query + letter)
    for letter2 in "abcdefghijklmnopqrstuvwxyz":
        auto_complete(base_query + letter + letter2)
  
#+end_src
**** (if time) guided practice: finding undocumented APIs
Yin, Leon. Finding Undocumented APIs. 24 Feb. 2023,
https://inspectelement.org/apis.html#tutorial.

Uses developer tools to reverse engineer google searches to examine
autocomplete results. 

**** BREAK
**** individual activity: explore how to bypass blockers
Try out some of these strategies: 
- [[https://scrapeops.io/web-scraping-playbook/403-forbidden-error-web-scraping/][How To Solve 403 Forbidden Errors When Web Scraping]]
- [[https://www.zenrows.com/blog/bypass-cloudflare-python][How to Bypass Cloudflare in Python]]
- [[https://www.zenrows.com/blog/curl-bypass-cloudflare#set-real-http-headers][4 Methods to Bypass Cloudflare with cURL in 2025]]

**** share what we've found
**** homework: /Compost Engineers/ chapters 3 & 4
Joana Varon and Lucía Egaña Rojas. Chapters 3 & 4 from /Compost
Engineers and Sus Saberes Lentos: A Manifest for Regenerative
Technologies/. Coding Rights, 2024,
https://codingrights.org/docs/compost_engineers.pdf.

Prompt: From the authors' proposals, what do you find useful or
surprising, and what do you have doubts about?

*** June 9, session 4: selenium
**** share homework, discuss reading
**** introduction to selenium

Install selenium

~conda install selenium~
~pip install selenium~

Install driver

https://sites.google.com/chromium.org/driver/getting-started?authuser=0

https://googlechromelabs.github.io/chrome-for-testing/files

Open ipython shell

#+begin_src python

# imports: driver, service, by
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

# variables to scrape site
url = 'https://doge.gov/'
d_path = './chromedriver-mac-arm64/chromedriver'
service = Service(executable_path=d_path)
driver = webdriver.Chrome(service = service)

# scrape site
driver.get(url)

### check inspector for element for each post: div.border-2

# scraping element using "find_element" function, which takes two
# arguments
card = driver.find_element(By.CSS_SELECTOR, "div.border-2")
card
card.text

# multiple elements with find_elements
cards = driver.find_elements(By.CSS_SELECTOR, "div.border-2")
cards

# get just the text
for i in cards:
    print(i.text)
len(cards)

### group challenge: write some code to extract the important
### information from these cards. you'll have to think about strategy:
### are you going to loop through the cards we already haveand take
### out the individual elements from each card, then save them to
### lists? Or will you re-scrape the content, specifically calling
### each item that we want?

#+end_src

**** BREAK

**** assignment: web scraping
Using either scrapy or selenium, scrape some data from a website that
you couldn't scrape before.
Bring that data to class. 

** unit 2 chat bots - 1.5 weeks
*** June 12, session 5: spaCy for processing text
**** share scraping assignments
**** intro to Python for cleaning text
- review replace() method and using Regex
- practice cleaning own dataset
**** the spaCy pipeline
**** BREAK
**** NER in spaCy
**** practice NER in dataset
**** homework: ACLU tech & privacy analysis write-up
Choose a recent topic from this page; write up analysis of what is
going on, and your opinion on the issue. How does the issue handle
privacy rights and ethical uses of data?
https://www.aclu.org/press-releases?issue=privacy-technology

*** June 16, session 6: spacy continued, intro to transformers 
**** share homeworks
**** pattern matching in spaCy
**** BREAK
**** introduction to huggingface
**** how to run inference
**** individual practice: explore tasks
**** homework: run a task on your own data

*** June 19, session 7: transformers continued
**** share homework
**** how to fine-tune a model
**** introductin to git and github
**** individual practice: fine-tune a model with own data
**** BREAK
**** in class: read and explore COMPAS algorithm
- “Can You Make AI Fairer than a Judge? Play Our Courtroom Algorithm
  Game.” MIT Technology Review,
  https://www.technologyreview.com/2019/10/17/75285/ai-fairer-than-judge-criminal-risk-assessment-algorithm/.
- recommended:
  - Groves, Lara, et al. “Auditing Work: Exploring the New York City
    Algorithmic Bias Audit Regime.” The 2024 ACM Conference on
    Fairness, Accountability, and Transparency, ACM, 2024, pp.
    1107–20. DOI.org (Crossref),
    https://doi.org/10.1145/3630106.3658959.

**** in class: end user algorithmic audit
- goals:
  - opportunity to discuss what makes something toxic
  - opportunity to examine how models treat toxicity

***** indie-label

[[https://github.com/StanfordHCI/indie-label][IndieLabel]]

Installations:
- use conda to create an env with python 3.8
- then use pip to install the packages
- if coming up against cython and/or surprise package errors, see:
  - [[https://stackoverflow.com/questions/77490435/attributeerror-cython-sources][AttributeError: cython_sources [duplicate]]]
  - [[https://stackoverflow.com/questions/65679417/getting-errors-while-installing-surprise-package][Getting errors while installing Surprise package]]
  - [[https://stackoverflow.com/questions/79374322/importerror-cannot-import-name-cached-download-from-huggingface-hub][ImportError: cannot import name 'cached_download' from 'huggingface_hub']]

#+begin_src console

  # create a constraint to avoid cython
  echo "cython<3" > /tmp/constraint.txt 
  PIP_CONSTRAINT=/tmp/constraint.txt pip install -r requirements.txt

  # install scikit-surprise separately with conda
  conda install -c conda-forge scikit-surprise

  # error when running server.py
  pip install huggingface-hub==0.25.2

#+end_src

Audit instructions:
- open a blank document for note-taking
- complete the questionnaire to get your model in the "okay" range. 
  - as you complete the questionnaire, make notes of deciding factors
    that made you choose if something is toxic or not toxic.
- explore the "auditing" tab, make notes on your findings. 

Report:
- provide 2 examples of choices that were difficult or that made you
  second guess yourself
- why was the choice difficult?

**** source code walkthrough: bias evaluation projects
- goals: learn to read complicated python code/projects; adapt
  pre-existing code to your own purposes/experimentation. 
***** biases-llm-reference-letters
https://github.com/uclanlp/biases-llm-reference-letters/tree/main?tab=readme-ov-file

Useful functions to count how many times certain words appear, male or
female words.

Also uses spacy to create lists of male and female nouns and
adjectives. 

**** assignment: dataset proposal
What is the dataset you'd like to create for your final project? Where
would you get the data, and how would you transform it? You can
consider tools from this class (like text generation, named entity
recognition, pattern matching), or you can consider other
possibilities for transforming your data. 1 page, double spaced.

** unit 3 social media bots - 1.5 weeks
*** June 23, session 8: twitter bots
[[https://thepythoncode.com/article/make-a-twitter-bot-in-python][Twitter bot with Python]] tutorial

**** configuring environments

#+begin_src console

mkdir met_women
cd met_women

conda create --name met_women

# Activate the virtual environment:
# - MacOS/Linux
conda activate met_women

#+end_src

#+begin_src console
touch .env
nano .env
#+end_src

#+begin_src python
# Consumer Keys > API Key and Secret
API_KEY=<your-API-key>
API_SECRET=<your-API-secret>

# Authentication Tokens > Access Token and Secret
ACCESS_TOKEN=<your-access-token>
ACCESS_TOKEN_SECRET=<your-access-token-secret>
#+end_src

#+begin_src console
touch .gitignore
nano .gitignore
#+end_src

#+begin_src
__pycache__
.env*
#+end_src

#+begin_src console
% pip install tweepy, requests, python-dotenv
#+end_src

**** tweet.py

touch tweet.py
code tweet.py

#+begin_src python
import os
import tweepy
import requests
from dotenv import load_dotenv
from random import randint

load_dotenv()

API_KEY = os.getenv("API_KEY")
API_SECRET = os.getenv("API_SECRET")
ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = os.getenv("ACCESS_TOKEN_SECRET")

client = tweepy.Client(
    consumer_key=API_KEY,
    consumer_secret=API_SECRET,
    access_token=ACCESS_TOKEN,
    access_token_secret=ACCESS_TOKEN_SECRET
)

def tweet_women_fact(tweepy_client):

   print('fetching women from the MET...')
   r1 = requests.get("https://collectionapi.metmuseum.org/public/collection/v1/search?q=woman")

   parsed = r1.json()

   number = randint(1, 100)

   obj_id = parsed['objectIDs'][number]

   r2 = requests.get(f"https://collectionapi.metmuseum.org/public/collection/v1/objects/{obj_id}")

   parsed = r2.json()

   if parsed['title'] != '':
       text = f"Title: {parsed['title']}"
   else:
       text = f"Title: Unknown"
   if parsed['artistDisplayName'] != '':
       artist = f"Artist: {parsed['artistDisplayName']}"
   else:
       artist = 'Artist: Unknown'
   if parsed['artistGender'] != '':
       gender = parsed['artistGender']
   else:
       gender = 'Gender: Unknown'

   image = parsed['objectURL']

   tweet_text = f"{text}, {artist}, {gender} {image}"
   print('tweeting women from the MET...')

   tweepy_client.create_tweet(text=tweet_text)

tweet_women_fact(client)

#+end_src

**** deploying our bot

Tutorials:
- [[https://www.python-engineer.com/posts/run-python-github-actions/][Adding secrets to github actions]]
- Automating a Twitter bot with GitHub Actions ([[https://github.com/gabrielbelolima/ttBot][github repo]])
  - [[https://medium.com/@gabrielbelolima/a-step-by-step-tutorial-part-1-3-71a7a8444b0cAutomating][part 1/3]]
  - [[https://medium.com/@gabrielbelolima/automating-a-twitter-bot-with-github-actions-a-step-by-step-tutorial-part-2-3-ebc4968a10ec][part 2/3]]
  - [[https://medium.com/@gabrielbelolima/automating-a-twitter-bot-with-github-actions-a-step-by-step-tutorial-part-3-3-df5d15b1e339][part 3/3]]

#+begin_src console
  mkdir .github
  mkdir .github/workflows
  cd .github/workflows
  touch actions.yml
  code actions.yml
#+end_src

#+begin_src yml

on:
  schedule:
#    - cron: '0 * * * *' # at top of every hour
    - cron: '0 0 * * *' # At 00:00 every day
  
  push: 

jobs:
  build:

    runs-on: ubuntu-latest

    steps:

      - name: checkout repo content
        uses: actions/checkout@v2 # checkout the repository content

      - name: setup python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10' # install the python version needed

      - name: install python packages
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: run scrupt 
        run: python tweet.py
        env: 
            API_KEY: ${{ secrets.API_KEY }}
            API_SECRET: ${{ secrets.API_SECRET }}
            ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}
            ACCESS_TOKEN_SECRET: ${{ secrets.ACCESS_TOKEN_SECRET }}
  
#+end_src

**** BREAK
**** group project: social media bot tutorial
- choose a social media app, like instagram, tiktok, linkedin, or
  another app of your choice.
- research some tutorials for scraping and/or creating a bot for that
  app. Make sure the tutorial is recent (in the last year, at
  minimum).
- with a partner, create a tutorial that you will use to teach your
  classmates how to scrape or create a bot on that app.
- tutorial should be written in markdown format, with each step
  described clearly, and code blocks to include code examples.
- you will present the tutorial like a lesson, where you walk your
  classmates through the process of using the tool.
- 20-30 minutes lesson. 

Resources:
- Yin, Piotr Sapiezynski and Leon. Browser Automation. 11 June 2023,
  https://inspectelement.org/browser_automation.html.
- Instagrapi, [[https://www.youtube.com/watch?v=cW7kMeOUr20][instagrapi tutorial]]
- [[https://www.geeksforgeeks.org/make-an-instagram-bot-with-python/][Make an Instagram Bot With Python]], Geeks for Geeks

**** make a plan for actions steps by next class
*** (online) June 26, session 9: group projects
**** share progress, next steps
**** BREAK
**** breakout work sessions
**** mini-conferences with me
*** (online) June 30, session 10: group projects continued
**** tutorial presentations
**** BREAK
**** introduction to git
**** introduce final project assignment
**** homework: project proposal
**** instagram
User: trans_phobia_
pass: supersecure

** unit 4 project workshops & presentations - 1 week
*** (online) July 3, final projects
- share progress
- project workshops
- mini conferences
*** (online) July 7, final project presentations
- presentations
