* INFO-697 How To Build A Bot!

** Course Information
Section: INFO-601-02
Semester: Spring 2025
Meeting Dates & Times: Tuesdays, 6:30-9:20 pm
Location: Pratt Manhattan Campus, room 704
Credits: 3
Prerequisites or other restrictions: None

** Instructor Contact Information
Filipa Calado, PhD (she)
Office Location: Pratt Manhattan Campus, room 602
Office Hours: Wednesdays 1:30-2:30pm and by appointment
Phone: 718-687-5194
Email: fcalado@pratt.edu
Zoom: https://pratt.zoom.us/my/fcalado 
Website: https://filipacalado.com 

** Course Description
This foundational course focuses on the intersection of people, information, and technology and the theoretical and conceptual foundations of the information field. Students will be introduced to ideas and concepts that will inform future specializations in their course of study and provide them with concrete strategies for ongoing professional growth and development in their area of interest.

*** The goals of this course are to:
introduce students to the major frameworks of the information field
help students to develop critical thinking, communication, research, management, presentation, and other skills required for successful completion of their programs and their professional growth
broaden students’ understanding of information professions and careers in the information field
						
*** By the end of this course, students will be able to:
- describe and critique historical, political, economic, social, and
  ethical aspects of information environments, including:
- various manifestations of information and internal and external
  factors affecting its production,  distribution, and consumption
- diversity of needs and characteristics of people, both individuals
  and communities, involved in  information creation, management, and use 
- foundational methods for assessing information environments and
  actors’ needs
- basic elements of project and institutional management
- express professional voice for analyzing and evaluating information,
  contextualizing their views, and communicating them effectively
- identify and value opportunities for continuing professional
  development

** Course Requirements
Sources and Materials
All readings are available freely online (linked via this syllabus) and/or on Canvas.

This syllabus is licensed under a Creative Commons Attribution
Non-Commercial Share Alike license.

*** Course Format
This is a seminar course, where each class is devoted to discussion of
the required materials for that week, as well as current events and
student work pertaining to those readings. You are required to review
the assigned materials in advance of each session and prepare to
discuss them in class.

Grading and Assignments All assignments are due on the date indicated,
and late work may receive reduced credit. Final grades will be awarded
as follows:

Grade Evaluation Quality Points per Credit

A Excellent 4.0

A- Excellent 3.7

B+ Very Good 3.3

B Good 3.0

B- Good 2.7

C+ Above Average 2.3

C Average 2.0

F Failure 0.0

WF Lack of Attendance 0.0

Note: C is the lowest passing grade for graduate students. 

Your grade in the course will be based on the following assignments: 
Critical Analysis paper (final project) 45%
Discussion posts 25% 
Fieldwork 20% 
AI activity 10% 

Discussion posts (25%) 
Throughout the course, you are expected to participate in online discussions of course topics. These posts should focus on course readings, as well as outside articles, resources, and events that are related to the course. You are required to make at least one post per week, and you are encouraged to make your post in response to a classmate’s post. Posts should be a short paragraph, about five to eight sentences (200 words max). 

You may use the discussion posts as a testing ground for close reading analysis. In that case, pick a moment from the reading that interests you or sticks out to you for some reason, and go into depth on elements of that moment which bear on your interpretation. For example, you might discuss how a certain video clip uses an emotional appeal in order to make its argument, or, focusing on language use, you might explore the implications and logics of a particular word (like “illegal” or “alien”) to describe a group of people. Make sure that your analysis is situated in the context of the piece and its audience (i.e., do not make universal claims like “all democrats think this way” based on one example).

Fieldwork (2 activities, 10% each) 
Throughout the semester, you should participate in at least two professional development opportunities and report back to the class through an informal discussion of up to five minutes, which may include questions from others. A writeup of your report should be submitted on Canvas. Examples of relevant activities include: 
attending professional events or live webinars 
watching a recorded lecture related to class topics 
other ideas, which you should feel free to discuss with me at any time 

Your written report can be written in an informal style, and should be about 300 words (one page double spaced, 12 point Times New Roman font, or Linux Libertine). 

All fieldwork must be completed and submitted before class on May 6.

AI Legal Opinion (10%) 
Our understanding of AI and legality are developing all the time. Pick a case from the “AI Lawsuits Worth Watching” article, and do a bit of research on the background of the case. Then, write up a summary of the case with your analysis. Use the following prompts to guide your document:
What is the main argument of the lawsuit? What are the counter arguments? 
Are there any aspects of the arguments that stand out to you or are surprising? How so? 
Which side has the stronger case, in your opinion? Why do you think so? 
Look at these documents, for some starting points:
Initial complaint
Memorandum of law (in reply to initial complaint) - legal reasoning by other side
Orders and Judgement - by the judge

Submit your “Legal Opinion” (900 - 1200 words, or 3-4 pages double spaced 12 pt Times New Roman font) before class on Week 3. You should use MLA to form your citations. 

Critical Analysis Paper (45%) - Link to rubric
In this assignment, you will engage in a critical analysis of a contemporary issue or discourse that is presented across multiple media formats. Specifically, you are tasked with choosing a popular issue (e.g., climate change, mis/dis-information, immigration, racial justice, political polarization, algorithmic bias, big tech, etc.) and analyzing how this issue is represented in two or three different examples from at least two different venues. These examples might include an essay, a news clip or article, a podcast, a social media post, a flyer, or any other form of public discourse, and a venue is a source or platform like The New York Times, FOX News, Twitter, Youtube, private blog, etc.

The key to this assignment is a close reading of each example you choose. Instead of offering a broad or general overview, you should dive deep into how the issue is framed, argued, and represented in each example. As you conduct your analysis, look at how rhetorical methods (emotions, logic, values) reveal underlying assumptions, and compare and contrast the strategies between your examples. Your analysis should also draw from readings, one of which must be from a text outside of class, to help corroborate your points. 

Length and Formatting
6-9 pages in length (double-spaced, 12 pt. Times New Roman font).
Proper MLA citation for all sources referenced.
Important dates
First example due: March 25
Second example due: April 8
Outline due: April 29
Draft due: May 6, 13
Final Paper due: May 15
** course overview
- May 27 - July 7:
- 10a - 1:50p (3:50/day, 7 hrs/week, 6 weeks)
- each session into blocks of: 1:50, 30 min break, 1:30 hours

** course schedule
*** unit 1 web crawling (surveillance) - 2 weeks
**** May 29, session 1: intro to Python & web scraping with bs4
***** python installations
- install python with anaconda
***** command line
- lesson: introduction to the command line
  - navigating
  - creating folders and files
  - using nano
  - curl requests
***** web scraping tools & ethics
- bs4, scrapy, selenium
- what is ethical, what is legal?
- read article on ICE,
  - quickwrite thoughts, discussion. 
***** BREAK
***** web scraping with bs4

****** Crash course on bs4: American Conservative articles

#+begin_src
from bs4 import BeautifulSoup

import requests

import lxml

soup = BeautifulSoup(requests.get('https://www.theamericanconservative.com/').content, 'lxml')

soup.title

### demonstrate how to use the inspector tool to find different parts of the page

# use the find() function to search by element and class
soup.find('', class_ = '')

# use the .text element to pull out just the text from the element
soup.find('', class_ = '').text

#### how would I get all of the headlines? what is the method?

# use the find_all() method to get all of the headlines
headlines = soup.find_all('', class_ = '')

headlines

for i in headlines:
    print(i.text)

### how would I save this information to a list? 

titles = []
for i in headlines:
    titles.append(i.text)
    
titles

### challenge: use these tools to get the summary blurb for each article. 10-15 minutes & share.

#+end_src

Challenge: write a function that scrapes all of the text from a
webapge. Make it so you can pass different URLs into the function, and
it will scrape a page for that URL. 

Advanced challenge: how would I scrape the article text?
hint: look at the URLs.
- make a list of URLs for every page.
- write a loop that goes through each one.
- turns that page into soup, grabs the article data, appends it to the
  list

Time to explore websites to scrape data from. Check if they are
scrapable first. 

***** (if time): freewrite/share
- Brainstorm what kind of data you are interested in working with.
  Look at some websites that may have that kind of data. Why are you
  interested in this data?
***** homework: find 2 scrapable sites
Find 2 websites to scrape. Make sure if they are scrapable with bs4.
Why are you interested in this data? What could you do with it?

**** June 2, session 2: scrapy & the scrapy shell
***** share websites that you found
***** introduction to scrapy shell
****** setting up environment
Create conda environment for scraping

Download and install scrapy. [[https://docs.scrapy.org/en/latest/intro/install.html#intro-install][Troubleshooting installtions]].

#+begin_src console
  conda install -c conda-forge scrapy
  pip install Scrapy
#+end_src

Introduction to scrapy shell

#+begin_src console
scrapy shell 'https://quotes.toscrape.com/page/1/'
response.css("title::text").get()
#+end_src

****** [[https://docs.scrapy.org/en/latest/intro/tutorial.html#extracting-data][Extracting data]] with css selectors

Syntax:

#+begin_src python
  # basic syntax for using element and class to get text
  # returns the entire element
  response.css("element.class::text")

  # getting the title elements
  response.css("title::text")

  # getting the quote elements
  response.css("span.text::text")

  # combine with get() to get just the text
  response.css("title::text").get()

  # and just the first instance
  response.css("span.text::text").get()

  # or with getall() to get a list
  response.css("span.text::text").getall()
#+end_src

Challenge: work to find the rest of the information on the page. Get
all the authors and the tags.

****** writing loops
- write a loop that saves our information to a loop. I will
  write the first two lines.
  - looping through a subset of the page.
  - using the print function
  - you will have to expand this loop.

#+begin_src python
  # first just looping through to print
  for quote in response.css("div.quote"):
       print(quote.css("span.text::text").get())

  # now saving it to a dictionary   
  quotes = {}
  for quote in response.css("div.quote"):
      quotes["text"] = quote.css("span.text::text").get()
#+end_src

Now, you will expand the loop to include author and tag information. 

***** (if time) explore a website you're interested in to get selectors
***** BREAK
***** scrapy project
****** starting new scrapy project

Install VS Code

Following tutorial on scrapy's [[https://docs.scrapy.org/en/latest/intro/tutorial.html][tutorial in the docs]]:

#+begin_src console
  # create your Scrapy project:
  scrapy startproject project_name
  cd project_name

  # see the directory structure
  tree
#+end_src

Create new spider manually (copy/paste code from [[https://docs.scrapy.org/en/latest/intro/tutorial.html#extracting-data-in-our-spider][extracting data in
our spider]]). 

#+begin_src python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/page/1/",
        "https://quotes.toscrape.com/page/2/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                "tags": quote.css("div.tags a.tag::text").getall(),
            }
#+end_src

Classes:
- classes are like templates, which you can customize.
- contain properties and functions.
- ~QuotesSpider~ class builds on the ~Spider~ class.
- check out the [[https://docs.scrapy.org/en/latest/topics/spiders.html][Spider class in the docs]].

Run the spider:

~scrapy crawl quotes~

Then store the scraped data:

~scrapy crawl quotes -O quotes.json~

***** (if time) individual activity: apply code to website from homework
- change the name, urls, and selectors.
- run the same command 

***** introduce book, read introduction together, discuss
***** homework: reading response /Compost Engineers/ chapters 1 & 2
Joana Varon and Lucía Egaña Rojas. Chapters 1 & 2 from /Compost
Engineers and Sus Saberes Lentos: A Manifest for Regenerative
Technologies/. Coding Rights, 2024,
https://codingrights.org/docs/compost_engineers.pdf.

Prompt: Pick an idea from the reading that interests you (either
because you agree with it, disagree with it, or are otherwise provoked
by it) and explain why.

**** June 5, session 3: blockers & XHR
***** share homework, discuss reading
***** scraping XHR
[[https://scrapism.lav.io/scraping-xhr/][Tutorial by Sam Lavigne]] on scraping Bing and Customs Border
Protection.

****** Exploring XHR from the command line

#+begin_src python
from bs4 import BeautifulSoup
import requests

query = "how can i"

url = (
    "https://www.bing.com/AS/Suggestions?pt=page.home&mkt=en-us&qry="
    + query
    + "&cp=9&csr=1&msbqf=false&pths=1&cvid=6AE710F2D778431589574CB8424EFF70"
)

response = requests.get(url)

response
dir(response)
response.text
response.content
response.json()

parsed = response.json()

# what kind of data structure?
# pull out the completions
parsed
parsed['s'][0]
parsed['s'][0]['q']
parsed['s'][1]['q']
parsed['s'][2]['q']

# write a loop that prints just the completions
for item in parsed['s']:
    print(item['q'])
#+end_src

****** Script for scraping XHR results.
- how & why to create a script
- how & why to write a function

Run the below. Pipe output through sort -u to sort the output of our
script and filter out duplicates.

#+begin_src console
  python bing_autocomplete.py | sort -u
#+end_src

#+begin_src python
from bs4 import BeautifulSoup
import requests

def auto_complete(query):
  url = (
      "https://www.bing.com/AS/Suggestions?pt=page.home&mkt=en-us&qry="
      + query
      + "&cp=10&cvid=B8D86CB090A240A196E4867715E40B15"
  )
  response = requests.get(url)
  soup = BeautifulSoup(response.text, "html.parser")
  items = soup.select("li")
  for item in items:
      print(item.text)

base_query = "How can I "
for letter in "abcdefghijklmnopqrstuvwxyz":
    auto_complete(base_query + letter)
    for letter2 in "abcdefghijklmnopqrstuvwxyz":
        auto_complete(base_query + letter + letter2)
  
#+end_src
***** (if time) guided practice: finding undocumented APIs
Yin, Leon. Finding Undocumented APIs. 24 Feb. 2023,
https://inspectelement.org/apis.html#tutorial.

Uses developer tools to reverse engineer google searches to examine
autocomplete results. 

***** BREAK
***** individual activity: explore how to bypass blockers
Try out some of these strategies: 
- [[https://scrapeops.io/web-scraping-playbook/403-forbidden-error-web-scraping/][How To Solve 403 Forbidden Errors When Web Scraping]]
- [[https://www.zenrows.com/blog/bypass-cloudflare-python][How to Bypass Cloudflare in Python]]
- [[https://www.zenrows.com/blog/curl-bypass-cloudflare#set-real-http-headers][4 Methods to Bypass Cloudflare with cURL in 2025]]

***** share what we've found
***** homework: /Compost Engineers/ chapters 3 & 4
Joana Varon and Lucía Egaña Rojas. Chapters 3 & 4 from /Compost
Engineers and Sus Saberes Lentos: A Manifest for Regenerative
Technologies/. Coding Rights, 2024,
https://codingrights.org/docs/compost_engineers.pdf.

Prompt: From the authors' proposals, what do you find useful or
surprising, and what do you have doubts about?

**** June 9, session 4: selenium
***** share homework, discuss reading
***** introduction to selenium

Install selenium

~conda install selenium~
~pip install selenium~

Install driver

https://sites.google.com/chromium.org/driver/getting-started?authuser=0

https://googlechromelabs.github.io/chrome-for-testing/files

Open ipython shell

#+begin_src python

# imports: driver, service, by
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

# variables to scrape site
url = 'https://doge.gov/'
d_path = './chromedriver-mac-arm64/chromedriver'
service = Service(executable_path=d_path)
driver = webdriver.Chrome(service = service)

# scrape site
driver.get(url)

### check inspector for element for each post: div.border-2

# scraping element using "find_element" function, which takes two
# arguments
card = driver.find_element(By.CSS_SELECTOR, "div.border-2")
card
card.text

# multiple elements with find_elements
cards = driver.find_elements(By.CSS_SELECTOR, "div.border-2")
cards

# get just the text
for i in cards:
    print(i.text)
len(cards)

### group challenge: write some code to extract the important
### information from these cards. you'll have to think about strategy:
### are you going to loop through the cards we already haveand take
### out the individual elements from each card, then save them to
### lists? Or will you re-scrape the content, specifically calling
### each item that we want?

#+end_src

***** BREAK

***** assignment: web scraping
Using either scrapy or selenium, scrape some data from a website that
you couldn't scrape before.
Bring that data to class. 

*** unit 2 chat bots (bias) - 1.5 weeks
**** June 12, session 5: spaCy for processing text
***** share scraping assignments
***** intro to Python for cleaning text
- review replace() method and using Regex
- practice cleaning own dataset
***** the spaCy pipeline
***** BREAK
***** NER in spaCy
***** practice NER in dataset
***** homework: ACLU tech & privacy analysis write-up
Choose a recent topic from this page; write up analysis of what is
going on, and your opinion on the issue. How does the issue handle
privacy rights and ethical uses of data?
https://www.aclu.org/press-releases?issue=privacy-technology

**** June 16, session 6: spacy continued, intro to transformers 
***** share homeworks
***** pattern matching in spaCy
***** BREAK
***** introduction to huggingface
***** how to run inference
***** individual practice: explore tasks
***** homework: run a task on your own data

**** June 19, session 7: transformers and bias
***** share homework
***** how to fine-tune a model
***** introductin to git and github
***** individual practice: fine-tune a model with own data
***** BREAK
***** in class: read and explore COMPAS algorithm
- “Can You Make AI Fairer than a Judge? Play Our Courtroom Algorithm
  Game.” MIT Technology Review,
  https://www.technologyreview.com/2019/10/17/75285/ai-fairer-than-judge-criminal-risk-assessment-algorithm/.
- recommended:
  - Groves, Lara, et al. “Auditing Work: Exploring the New York City
    Algorithmic Bias Audit Regime.” The 2024 ACM Conference on
    Fairness, Accountability, and Transparency, ACM, 2024, pp.
    1107–20. DOI.org (Crossref),
    https://doi.org/10.1145/3630106.3658959.

***** in class: end user algorithmic audit
- goals:
  - opportunity to discuss what makes something toxic
  - opportunity to examine how models treat toxicity

****** indie-label

[[https://github.com/StanfordHCI/indie-label][IndieLabel]]

Installations:
- use conda to create an env with python 3.8
- then use pip to install the packages
- if coming up against cython and/or surprise package errors, see:
  - [[https://stackoverflow.com/questions/77490435/attributeerror-cython-sources][AttributeError: cython_sources [duplicate]]]
  - [[https://stackoverflow.com/questions/65679417/getting-errors-while-installing-surprise-package][Getting errors while installing Surprise package]]
  - [[https://stackoverflow.com/questions/79374322/importerror-cannot-import-name-cached-download-from-huggingface-hub][ImportError: cannot import name 'cached_download' from 'huggingface_hub']]

#+begin_src console

  # create a constraint to avoid cython
  echo "cython<3" > /tmp/constraint.txt 
  PIP_CONSTRAINT=/tmp/constraint.txt pip install -r requirements.txt

  # install scikit-surprise separately with conda
  conda install -c conda-forge scikit-surprise

  # error when running server.py
  pip install huggingface-hub==0.25.2

#+end_src

Audit instructions:
- open a blank document for note-taking
- complete the questionnaire to get your model in the "okay" range. 
  - as you complete the questionnaire, make notes of deciding factors
    that made you choose if something is toxic or not toxic.
- explore the "auditing" tab, make notes on your findings. 

Report:
- provide 2 examples of choices that were difficult or that made you
  second guess yourself
- why was the choice difficult?

***** source code walkthrough: bias evaluation projects
- goals: learn to read complicated python code/projects; adapt
  pre-existing code to your own purposes/experimentation. 
****** biases-llm-reference-letters
https://github.com/uclanlp/biases-llm-reference-letters/tree/main?tab=readme-ov-file

Useful functions to count how many times certain words appear, male or
female words.

Also uses spacy to create lists of male and female nouns and
adjectives. 

***** assignment: dataset proposal
What is the dataset you'd like to create for your final project? Where
would you get the data, and how would you transform it? You can
consider tools from this class (like text generation, named entity
recognition, pattern matching), or you can consider other
possibilities for transforming your data. 1 page, double spaced.

*** unit 3 social media bots (algorithms) - 1.5 weeks
**** June 23, session 8: twitter bots
[[https://thepythoncode.com/article/make-a-twitter-bot-in-python][Twitter bot with Python]] tutorial

***** configuring environments

#+begin_src console

mkdir met_women
cd met_women

conda create --name met_women

# Activate the virtual environment:
# - MacOS/Linux
conda activate met_women

#+end_src

#+begin_src console
touch .env
nano .env
#+end_src

#+begin_src python
# Consumer Keys > API Key and Secret
API_KEY=<your-API-key>
API_SECRET=<your-API-secret>

# Authentication Tokens > Access Token and Secret
ACCESS_TOKEN=<your-access-token>
ACCESS_TOKEN_SECRET=<your-access-token-secret>
#+end_src

#+begin_src console
touch .gitignore
nano .gitignore
#+end_src

#+begin_src
__pycache__
.env*
#+end_src

#+begin_src console
% pip install tweepy, requests, python-dotenv
#+end_src

***** tweet.py

touch tweet.py
code tweet.py

#+begin_src python
import os
import tweepy
import requests
from dotenv import load_dotenv
from random import randint

load_dotenv()

API_KEY = os.getenv("API_KEY")
API_SECRET = os.getenv("API_SECRET")
ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = os.getenv("ACCESS_TOKEN_SECRET")

client = tweepy.Client(
    consumer_key=API_KEY,
    consumer_secret=API_SECRET,
    access_token=ACCESS_TOKEN,
    access_token_secret=ACCESS_TOKEN_SECRET
)

def tweet_women_fact(tweepy_client):

   print('fetching women from the MET...')
   r1 = requests.get("https://collectionapi.metmuseum.org/public/collection/v1/search?q=woman")

   parsed = r1.json()

   number = randint(1, 100)

   obj_id = parsed['objectIDs'][number]

   r2 = requests.get(f"https://collectionapi.metmuseum.org/public/collection/v1/objects/{obj_id}")

   parsed = r2.json()

   if parsed['title'] != '':
       text = f"Title: {parsed['title']}"
   else:
       text = f"Title: Unknown"
   if parsed['artistDisplayName'] != '':
       artist = f"Artist: {parsed['artistDisplayName']}"
   else:
       artist = 'Artist: Unknown'
   if parsed['artistGender'] != '':
       gender = parsed['artistGender']
   else:
       gender = 'Gender: Unknown'

   image = parsed['objectURL']

   tweet_text = f"{text}, {artist}, {gender} {image}"
   print('tweeting women from the MET...')

   tweepy_client.create_tweet(text=tweet_text)

tweet_women_fact(client)

#+end_src

***** deploying our bot

Tutorials:
- [[https://www.python-engineer.com/posts/run-python-github-actions/][Adding secrets to github actions]]
- Automating a Twitter bot with GitHub Actions ([[https://github.com/gabrielbelolima/ttBot][github repo]])
  - [[https://medium.com/@gabrielbelolima/a-step-by-step-tutorial-part-1-3-71a7a8444b0cAutomating][part 1/3]]
  - [[https://medium.com/@gabrielbelolima/automating-a-twitter-bot-with-github-actions-a-step-by-step-tutorial-part-2-3-ebc4968a10ec][part 2/3]]
  - [[https://medium.com/@gabrielbelolima/automating-a-twitter-bot-with-github-actions-a-step-by-step-tutorial-part-3-3-df5d15b1e339][part 3/3]]

#+begin_src console
  mkdir .github
  mkdir .github/workflows
  cd .github/workflows
  touch actions.yml
  code actions.yml
#+end_src

#+begin_src yml

on:
  schedule:
#    - cron: '0 * * * *' # at top of every hour
    - cron: '0 0 * * *' # At 00:00 every day
  
  push: 

jobs:
  build:

    runs-on: ubuntu-latest

    steps:

      - name: checkout repo content
        uses: actions/checkout@v2 # checkout the repository content

      - name: setup python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10' # install the python version needed

      - name: install python packages
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: run scrupt 
        run: python tweet.py
        env: 
            API_KEY: ${{ secrets.API_KEY }}
            API_SECRET: ${{ secrets.API_SECRET }}
            ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}
            ACCESS_TOKEN_SECRET: ${{ secrets.ACCESS_TOKEN_SECRET }}
  
#+end_src

***** BREAK
***** group project: social media bot tutorial
- choose a social media app, like instagram, tiktok, linkedin, or
  another app of your choice.
- research some tutorials for scraping and/or creating a bot for that
  app. Make sure the tutorial is recent (in the last year, at
  minimum).
- with a partner, create a tutorial that you will use to teach your
  classmates how to scrape or create a bot on that app.
- tutorial should be written in markdown format, with each step
  described clearly, and code blocks to include code examples.
- you will present the tutorial like a lesson, where you walk your
  classmates through the process of using the tool.
- 20-30 minutes lesson. 

Resources:
- Yin, Piotr Sapiezynski and Leon. Browser Automation. 11 June 2023,
  https://inspectelement.org/browser_automation.html.
- Instagrapi, [[https://www.youtube.com/watch?v=cW7kMeOUr20][instagrapi tutorial]]
- [[https://www.geeksforgeeks.org/make-an-instagram-bot-with-python/][Make an Instagram Bot With Python]], Geeks for Geeks

***** make a plan for actions steps by next class
**** (online) June 26, session 9: group project
***** share progress, next steps
***** BREAK
***** breakout work sessions
***** mini-conferences with me
**** (online) June 30, session 10: group projects continued
***** tutorial presentations
***** BREAK
***** introduction to git
***** introduce final project assignment
***** homework: project proposal
***** instagram
User: trans_phobia_
pass: supersecure

*** unit 4 project workshops & presentations - 1 week
**** (online) July 3
- share progress
- project workshops
- mini conferences
**** (online) July 7
- presentations
** assignments
*** participation (30%)
- includes daily homework
*** unit assignments (30%)
- includes the assignment at the end of units 1-3
*** final project: some bot! (40%)
- final project that takes some data from web scraping or APIs, and
  uses it as the content for a bot.
- bot to be automated and published on github. 
** recommended readings

*** on data gathering and web scraping
- Dodge, Jesse, et al. “Documenting Large Webtext Corpora: A Case
  Study on the Colossal Clean Crawled Corpus.” Proceedings of the 2021
  Conference on Empirical Methods in Natural Language Processing,
  edited by Marie-Francine Moens et al., Association for Computational
  Linguistics, 2021, pp. 1286–305. ACLWeb,
  https://doi.org/10.18653/v1/2021.emnlp-main.98.
- Jo, Eun Seo, and Timnit Gebru. “Lessons from Archives: Strategies
  for Collecting Sociocultural Data in Machine Learning.” Proceedings
  of the 2020 Conference on Fairness, Accountability, and
  Transparency, Association for Computing Machinery, 2020, pp. 306–16.
  ACM Digital Library, https://doi.org/10.1145/3351095.3372829.
- Chan, Anita Say. Predatory Data: Eugenics in Big Tech and Our Fight
  for an Independent Future. University of California Press, 2025.
  library.oapen.org, https://doi.org/10.1525/luminos.215.
- Métraux, Julia. “Eugenics Isn’t Dead—It’s Thriving in Tech.” Mother
  Jones,
  https://www.motherjones.com/politics/2025/01/eugenics-isnt-dead-its-thriving-in-tech/.
  Accessed 14 Feb. 2025.

*** on machine learning
- Alammar, Jay. The Illustrated BERT, ELMo, and Co. (How NLP Cracked
  Transfer Learning). https://jalammar.github.io/illustrated-bert/.
  Accessed 14 Apr. 2025.
- Alammar, Jay. The Illustrated DeepSeek-R1. 10 Feb. 2025,
  https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1.

*** case studies of algorithmic bias & audits
- Hada, Rishav, et al. “Akal Badi Ya Bias: An Exploratory Study of
  Gender Bias in Hindi Language Technology.” The 2024 ACM Conference
  on Fairness, Accountability, and Transparency, ACM, 2024, pp.
  1926–39. DOI.org (Crossref),
  https://doi.org/10.1145/3630106.3659017.
- Gajjala, Radhika, et al. “Get the Hammer out! Breaking Computational
  Tools for Feminist, Intersectional ‘Small Data’ Research.” Journal
  of Digital Social Research, vol. 6, no. 2, 2, May 2024, pp. 9–26.
  jdsr.se, https://doi.org/10.33621/jdsr.v6i2.193.
- Tang, Ningjing, et al. “AI Failure Cards: Understanding and
  Supporting Grassroots Efforts to Mitigate AI Failures in Homeless
  Services.” The 2024 ACM Conference on Fairness, Accountability, and
  Transparency, ACM, 2024, pp. 713–32. DOI.org (Crossref),
  https://doi.org/10.1145/3630106.3658935.
- Groves, Lara, et al. “Auditing Work: Exploring the New York City
  Algorithmic Bias Audit Regime.” The 2024 ACM Conference on Fairness,
  Accountability, and Transparency, ACM, 2024, pp. 1107–20. DOI.org
  (Crossref), https://doi.org/10.1145/3630106.3658959.
- Costanza-Chock, Sasha, et al. “Who Audits the Auditors?
  Recommendations from a Field Scan of the Algorithmic Auditing
  Ecosystem.” Proceedings of the 2022 ACM Conference on Fairness,
  Accountability, and Transparency, Association for Computing
  Machinery, 2022, pp. 1571–83. ACM Digital Library,
  https://doi.org/10.1145/3531146.3533213.


